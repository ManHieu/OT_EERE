Dataset: HiEve
Sample rate = 0.4
Random_state: 7890
Hyperparams: 
 {'lr': 0.0005, 'OT_max_iter': 50, 'encoder_lr': 3e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 30, 'use_pretrained_wemb': False, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 7890, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/hievents_v2/', 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/roberta-large', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/roberta-large', 'scratch_tokenizer_name_or_path': 'datasets/hievents_v2/tokenizer.pkl', 'n_fold': '1', 'num_labels': '4', 'max_seq_length': '512', 'loss_weights': [18.51761517615176, 19.635057471264368, 42.17901234567901, 1.1476318441383944]}
F1: 0.6536373507057546 - 0.0 
P: 0.5873170731707317 - 0.0 
R: 0.7368421052631579 - 0.0 
-------------------- 

Dataset: HiEve 
Sample rate = 0.2
Random_state: 1652
Hyperparams: 
 {'lr': 8e-05, 'OT_max_iter': 50, 'encoder_lr': 3e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 30, 'use_pretrained_wemb': False, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1652, 'gcn_num_layers': 3, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/hievents_v2/', 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/roberta-large', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/roberta-large', 'scratch_tokenizer_name_or_path': 'datasets/hievents_v2/tokenizer.pkl', 'n_fold': '1', 'num_labels': '4', 'max_seq_length': '512', 'loss_weights': [18.51761517615176, 19.635057471264368, 42.17901234567901, 1.1476318441383944]}
F1: 0.6944818304172273 - 0.0 
P: 0.6224366706875754 - 0.0 
R: 0.7853881278538812 - 0.0 
-------------------- 

Dataset: ESL
Random_state: 1
Hyperparams: 
 {'lr': 0.0003, 'OT_max_iter': 50, 'encoder_lr': 1e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 50, 'use_pretrained_wemb': False, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/EventStoryLine', 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/roberta-large', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/roberta-large', 'scratch_tokenizer_name_or_path': 'datasets/EventStoryLine/0/tokenizer.pkl', 'n_fold': '5', 'num_labels': '2', 'max_seq_length': '512', 'loss_weights': [0.8333333333333334, 0.16666666666666666]}
F1: 0.6473903201968018 - 0.0 
P: 0.5808170951414797 - 0.0 
R: 0.7346153846153847 - 0.0 
-------------------- 

Dataset: subevent_mulerx 
Random_state: 1741
Hyperparams: 
 {'lr': 0.0001, 'OT_max_iter': 50, 'encoder_lr': 3e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 20, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-en-10', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [27.82191780821918, 27.82191780821918, 1.0774535809018568], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/mBERT-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/mBERT-base'}
F1: 0.4052287581699347 - 0.0 
P: 0.3309608540925267 - 0.0 
R: 0.5224719101123596 - 0.0 

