Dataset: HiEve
Sample rate = 0.4
Random_state: 7890
Hyperparams: 
 {'lr': 0.0005, 'OT_max_iter': 50, 'encoder_lr': 3e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 30, 'use_pretrained_wemb': False, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 7890, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/hievents_v2/', 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/roberta-large', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/roberta-large', 'scratch_tokenizer_name_or_path': 'datasets/hievents_v2/tokenizer.pkl', 'n_fold': '1', 'num_labels': '4', 'max_seq_length': '512', 'loss_weights': [18.51761517615176, 19.635057471264368, 42.17901234567901, 1.1476318441383944]}
F1: 0.6536373507057546 - 0.0 
P: 0.5873170731707317 - 0.0 
R: 0.7368421052631579 - 0.0 
-------------------- 

Dataset: HiEve 
Sample rate = 0.2
Random_state: 1652
Hyperparams: 
 {'lr': 8e-05, 'OT_max_iter': 50, 'encoder_lr': 3e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 30, 'use_pretrained_wemb': False, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1652, 'gcn_num_layers': 3, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/hievents_v2/', 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/roberta-large', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/roberta-large', 'scratch_tokenizer_name_or_path': 'datasets/hievents_v2/tokenizer.pkl', 'n_fold': '1', 'num_labels': '4', 'max_seq_length': '512', 'loss_weights': [18.51761517615176, 19.635057471264368, 42.17901234567901, 1.1476318441383944]}
F1: 0.6944818304172273 - 0.0 
P: 0.6224366706875754 - 0.0 
R: 0.7853881278538812 - 0.0 
-------------------- 

Dataset: ESL
Random_state: 1
Hyperparams: 
 {'lr': 0.0003, 'OT_max_iter': 50, 'encoder_lr': 1e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 50, 'use_pretrained_wemb': False, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/EventStoryLine', 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/roberta-large', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/roberta-large', 'scratch_tokenizer_name_or_path': 'datasets/EventStoryLine/0/tokenizer.pkl', 'n_fold': '5', 'num_labels': '2', 'max_seq_length': '512', 'loss_weights': [0.8333333333333334, 0.16666666666666666]}
F1: 0.6473903201968018 - 0.0 
P: 0.5808170951414797 - 0.0 
R: 0.7346153846153847 - 0.0 
-------------------- 

-------------------- 
Dataset: subevent_mulerx 
mBERT - da
Random_state: 1741
Hyperparams: 
 {'lr': 5e-05, 'OT_max_iter': 50, 'encoder_lr': 5e-05, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 50, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-da-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [0.47619047619047616, 0.47619047619047616, 0.047619047619047616], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/mBERT-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/mBERT-base'}
F1: 0.48929663608562685 - 0.0 
P: 0.41308089500860584 - 0.0 
R: 0.6 - 0.0 
-------------------- 

Dataset: subevent_mulerx 
XLM-R - da
Random_state: 1741
Hyperparams: 
 {'lr': 5e-05, 'OT_max_iter': 50, 'encoder_lr': 5e-05, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 50, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-da-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [0.47619047619047616, 0.47619047619047616, 0.047619047619047616], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/XLM-R-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/XLM-R-base'}
F1: 0.500475737392959 - 0.0 
P: 0.4039938556067588 - 0.0 
R: 0.6575 - 0.0 
--------------------  

Dataset: subevent_mulerx 
mBERT - en
Random_state: 1741
Hyperparams: 
 {'lr': 7e-05, 'OT_max_iter': 50, 'encoder_lr': 1e-05, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 30, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-en-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [0.47619047619047616, 0.47619047619047616, 0.047619047619047616], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/mBERT-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/mBERT-base'}
F1: 0.4931921331316188 - 0.0 
P: 0.5344262295081967 - 0.0 
R: 0.45786516853932585 - 0.0 
---------------------

Dataset: subevent_mulerx 
mBERT - tr
Random_state: 1741
Hyperparams: 
 {'lr': 7e-05, 'OT_max_iter': 50, 'encoder_lr': 7e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 20, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-tr-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [4.0, 4.0, 1.0], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/mBERT-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/mBERT-base'}
F1: 0.5011600928074247 - 0.0 
P: 0.4576271186440678 - 0.0 
R: 0.5538461538461539 - 0.0 
-------------------- 

Dataset: subevent_mulerx
mBERT-ur 
Random_state: 1741
Hyperparams: 
 {'lr': 5e-05, 'OT_max_iter': 50, 'encoder_lr': 1e-05, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 30, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-ur-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [4.0, 4.0, 1.0], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/mBERT-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/mBERT-base'}
F1: 0.5219206680584552 - 0.0 
P: 0.5230125523012552 - 0.0 
R: 0.5208333333333334 - 0.0 
-------------------- 

Dataset: subevent_mulerx 
XLM-R - en
Random_state: 1741
Hyperparams: 
 {'lr': 7e-05, 'OT_max_iter': 50, 'encoder_lr': 1e-06, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 50, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-en-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [4.0, 4.0, 1.0], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/XLM-R-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/XLM-R-base'}
F1: 0.49470899470899476 - 0.0 
P: 0.4675 - 0.0 
R: 0.5252808988764045 - 0.0 
--------------------

Dataset: subevent_mulerx 
mBERT - es
Random_state: 1741
Hyperparams: 
 {'lr': 7e-05, 'OT_max_iter': 50, 'encoder_lr': 1e-05, 'batch_size': 8, 'warmup_ratio': 0.1, 'num_epoches': 20, 'regular_loss_weight': 0.1, 'OT_loss_weight': 0.1, 'distance_emb_size': 0, 'seed': 1741, 'gcn_num_layers': 2, 'hidden_size': 768, 'rnn_num_layers': 1, 'fn_actv': 'leaky_relu', 'residual_type': 'addtive', 'data_dir': 'datasets/mulerx/subevent-es-20', 'n_fold': '1', 'num_labels': '3', 'max_seq_length': '512', 'loss_weights': [4.0, 4.0, 1.0], 'tokenizer': '/vinai/hieumdt/pretrained_models/tokenizers/mBERT-base', 'encoder_name_or_path': '/vinai/hieumdt/pretrained_models/models/mBERT-base'}
F1: 0.40460526315789475 - 0.0 
P: 0.3435754189944134 - 0.0 
R: 0.492 - 0.0 
-------------------- 
